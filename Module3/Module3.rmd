---
title: "Module3"
author: "Patrick Foster"
date: "`r Sys.Date()`"
output:
  pdf_document: default
bibliography: references.bib
---

```{r setup, include=FALSE, echo=F}
knitr::opts_chunk$set(fig.align="center", fig.pos="H", autodep=TRUE)
library(tidyverse)
library(tidymodels)
library(discrim) # for LDA and QDA
library(ggcorrplot)  # for correlation plot
library(GGally)  # scatterplot matrix
library(patchwork)  # for combining plots
library(probably)  # for threshold_perf
```

## 1 Differences between LDA and QDA

### 1.1

If a Bayes decision boundary is linear we expect a LDA to perform better on both the training and test sets.

### 1.2

If a Bayes decision boundary is non-linear we expect a QDA to perform better on the the training and test sets.

### 1.3

As the number of observations increases QDA becomes more accurate when compared to LDA, as the training set increases the variance of the classifier is not a major concern.

### 1.4

False: If the Bayes decision boundary is truly linear, the LDA will be the superior method the data. This is because the QDA will overfit and follow trends in the data that are not present.

## 2 NASA: Asteroid classification

### 2.1 Load and preprocess

```{r, include=T}

remove_columns <- c("Name", "Est Dia in M(min)",
    "Semi Major Axis", "Jupiter Tisserand Invariant",
    "Epoch Osculation", "Mean Motion", "Aphelion Dist",
    "Equinox", "Orbiting Body", "Orbit Determination Date",
    "Close Approach Date", "Epoch Date Close Approach",
    "Miss Dist.(Astronomical)", "Miles per hour")
asteroids <- read_csv("https://gedeck.github.io/DS-6030/datasets/nasa.csv", 
                      show_col_types = FALSE) %>%
    select(-all_of(remove_columns)) %>%
    select(-contains("Relative Velocity")) %>%
    select(-contains("Est Dia in KM")) %>%
    select(-contains("Est Dia in Feet")) %>%
    select(-contains("Est Dia in Miles")) %>%
    select(-contains("Miss Dist.(lunar)")) %>%
    select(-contains("Miss Dist.(kilometers)")) %>%
    select(-contains("Miss Dist.(miles)")) %>%
    distinct() %>%
    mutate(Hazardous = as.factor(Hazardous))
dim(asteroids)
```

This code reads in all of the info needed from the csv data set and cleans it up, removing the data that is not needed or is hard to work with.

### 2.2 Data split and model creation.

Before we start lets take a look at which variables should be included as predictors in the model. From a brief glance at the data we can remove the IDs and also perform a log transform on the minimum orbit intersection. Also Magnitude and Est Diameter in M are directly correlated so we can remove one, I am going to remove the Est Diameter.

```{r remove_ref_log, include=TRUE}
asteroids <- asteroids %>% 
  select(-contains("Neo Reference Id")) %>% 
  select(-contains("Orbit Id")) %>% 
  mutate(
    `Est Dia in M(max)`  = log(`Est Dia in M(max)`),
    `Minimum Orbit Intersection` = log(`Minimum Orbit Intersection`)
  ) %>% 
  relocate(Hazardous)

```

```{r visual, include=TRUE}
asteroids %>% 
    pivot_longer(!Hazardous, names_to='variable', values_to='value') %>%
ggplot(aes(y=value, x=Hazardous)) +
    geom_boxplot() +
    facet_wrap(~variable, scales='free')

```

We can see now that most of the variables seem to have some affect on the outcome. I also am going to remove Asc Node Longitude, Perihelion Arg, and Perihelion Time. Finally, Orbit Uncertainty should be changed to a factor, I am going create a new variable that classifies something with high or low uncertainty.

```{r}
#| fig.width: 4
#| fig.height: 3
#| out.width: 75%
asteroids %>% 
  ggplot(aes(x=`Orbit Uncertainity`, fill = Hazardous))+
  geom_bar(position = "dodge")
```

We have an interesting split of the data when looking at the Orbit Uncertainty, I am going to classify anything with an uncertainty level greater than 4 as "high" uncertainty.

```{r uncert, include=TRUE}
asteroids <- asteroids %>% 
  mutate(
    `Orbit Uncertainity` = ifelse(`Orbit Uncertainity`>=5, "high","low"),
    `Orbit Uncertainity`= factor(`Orbit Uncertainity`, levels = c("high","low"))
  )
```

```{r data_split, include= T}
set.seed(1)
asteroid_split <- initial_split(asteroids, prop = .8, strata = "Hazardous")
asteroid_train <- training(asteroid_split)
asteroid_holdout <- testing(asteroid_split)
```

The data is now split into 80% training data with 20% holdout data, with a strata value of "Hazardous". This makes sure there are equal proportions of the Hazardous response variable in both training sets. As evidenced by the plot below.

```{r data_split_even, include=TRUE}
#| out.width: 75%
g1 <- asteroid_train %>% 
  ggplot(aes(x=Hazardous, fill = Hazardous))+
  geom_bar()+
  labs(title = "Train")

g2 <- asteroid_holdout %>% 
  ggplot(aes(x=Hazardous, fill = Hazardous))+
  geom_bar()+
  labs(title = "Holdout")

p1 <- g1 + g2

p1 +
  plot_annotation(title = "Training and Holdout Set class split")+
  plot_layout(guides = "collect")& theme(legend.position = "right")
```

```{r models, include= T}

formula <- Hazardous ~ +`Absolute Magnitude`+`Orbit Uncertainity`+
  `Minimum Orbit Intersection`+Eccentricity+Inclination+
    `Orbital Period`+`Perihelion Distance`+`Mean Anomaly`

reference_model <- null_model(mode="classification") %>%
    set_engine("parsnip") %>%
    fit(formula, asteroid_train)

logreg_model <- logistic_reg(mode="classification", engine="glm") %>%
    fit(formula, asteroid_train)

lda_model <- discrim_linear(mode="classification", engine="MASS") %>%
    fit(formula, asteroid_train)

qda_model <- discrim_quad(mode="classification", engine="MASS") %>%
    fit(formula, asteroid_train)
```

### 2.3 Evaluate the Performance

Taking the code that was used in the class and adapting it for these purposes we can get a good idea of the models performance. Making sure to change the event_level of the performance, because here we have "pred.FALSE" then "pred.TRUE" and we want to look at the TRUE predictions.

```{r utility-functions}
#| echo: FALSE
calculate_metrics <- function(model, train, holdout, model_name) {
    bind_rows(
        # Accuracy of holdout set
        bind_cols(
            model=model_name,
            dataset="holdout",
            metrics(model %>% augment(holdout), truth=Hazardous, 
                    estimate=.pred_class),
        ),
        # AUC of ROC curve of holdout set
        bind_cols(
            model=model_name,
            dataset="holdout",
            roc_auc(model %>% augment(holdout), Hazardous, .pred_TRUE, 
                    event_level="second"),
        ),
    )
}

metrics_table <- function(all_metrics, caption) {
    all_metrics %>%
        pivot_wider(names_from=.metric, values_from=.estimate) %>%
        select(-.estimator) %>%
        knitr::kable(caption=caption, digits=3) %>%
        kableExtra::kable_styling(full_width=FALSE)
}
```

```{r metrics-table}
all_metrics <- bind_rows(
    calculate_metrics(reference_model, asteroid_train, asteroid_holdout, 
                      "reference"),
    calculate_metrics(logreg_model, asteroid_train, asteroid_holdout, "logreg"),
    calculate_metrics(lda_model, asteroid_train, asteroid_holdout, "LDA"),
    calculate_metrics(qda_model, asteroid_train, asteroid_holdout, "QDA"),
)
all_metrics <- all_metrics %>% arrange(dataset)
metrics_table(all_metrics, "Metrics for the classification models")
```

```{r metrics-graph}
#| fig.cap: Metrics of the classification models determined using the training and the holdout sets
#| fig.width: 10
#| fig.height: 3
#| out.width: 100%
ggplot(all_metrics, aes(x=.estimate, y=model, color=dataset)) +
    geom_point() +
    facet_wrap(~ .metric, scale="free_x")
```

Here we can see that for the holdout data the QDA has the best accuracy and has the best KAP, measurements. With the ROC_AUC we can see that the model needs be tuned pick a different value for the probability thresholds.

### 2.4 Plot the ROC curves for each of the Models

We can also co-opt the code from class to make nice visuals for the ROC curves.

```{r roc-curves}
#| fig.cap: ROC curves of the four classification models determined using the holdout set.
#| fig.width: 6
#| fig.height: 6
#| out.width: 75%
get_roc_plot <- function(model, data, model_name) {
    roc_data <- model %>%
        augment(data) %>%
        roc_curve(truth=Hazardous, .pred_TRUE, event_level="second")
    g <- autoplot(roc_data) +
        labs(title=model_name)
    return(g)
}

g1 <- get_roc_plot(reference_model, asteroid_holdout, "Null model")
g2 <- get_roc_plot(logreg_model, asteroid_holdout, "Logistic regression")
g3 <- get_roc_plot(lda_model, asteroid_holdout, "LDA")
g4 <- get_roc_plot(qda_model, asteroid_holdout, "QDA")
(g1 + g2) / (g3 + g4)
```

Here we can see that all of the models perform better than the null model when using the ROC curve. However the QDA seems to have the greatest AUC and is therefore the best model.

### 2.5 Single plot for ROC curves

First I am going to modify our above code to return the roc_data instead of the autoplot.

```{r}
get_roc_data <- function(model, data, model_name) {
    roc_data <- model %>%
        augment(data) %>%
        roc_curve(truth=Hazardous, .pred_TRUE, event_level="second")
    return(roc_data)
}
```

Then I am going to bind the returned tibbles together and use ggplot to create a plot with all four ROC curves.

```{r fourInOne, include=TRUE}
#| out.width: 75%  
#| fig.cap: ROC curves of the four classification models determined using the holdout set.

Null <- get_roc_data(reference_model, asteroid_holdout, "Null model")
Logistic <- get_roc_data(logreg_model, asteroid_holdout, "Logistic regression")
LDA <- get_roc_data(lda_model, asteroid_holdout, "LDA")
QDA <- get_roc_data(qda_model, asteroid_holdout, "QDA")

roc_data <- bind_rows(
    Null %>% mutate(Model="NULL"),
    Logistic %>% mutate(Model="Logistic"),
    LDA %>% mutate(Model="LDA"),
    QDA %>% mutate(Model="QDA")
  )

r1 <- roc_data %>% 
  ggplot(aes(x=(1-specificity),y=sensitivity, color = Model))+
    geom_line()+
    geom_abline(linetype = "dashed")

r1
```  


With this visual we can see that QDA definitely has the most area underneath the curve. We can also see that the LDA and Logistic models perform similarly.

### 2.6 Calculate the F-measure

In order to calculate the threshold that optimizes based off the performance metric of f_measure we have to do something like the following for every model.

```{r threshold, include=TRUE}
f <- lda_model %>% augment(asteroid_train)

probably::threshold_perf(f, Hazardous, .pred_TRUE,
    thresholds=seq(0.05, 0.95, 0.01), event_level="second",
    metrics=metric_set(f_meas))
```

We would then sort the model by the estimator and choose the best one. We can take the code from class that bases the selection off of accuracy and modify it for our needs.

```{r threshold-scan}
#| fig.cap: F-Measure as a function of threshold for the three classification models
#| fig.width: 12
#| fig.height: 4
#| out.width: 100%
threshold_scan <- function(model, data, model_name) {
    threshold_data <- model %>%
        augment(data) %>%
        probably::threshold_perf(Hazardous, .pred_TRUE,
            thresholds=seq(0.05, 0.95, 0.01), event_level="second",
            metrics=metric_set(f_meas))
    opt_threshold <- threshold_data %>%
        arrange(-.estimate) %>%
        first()
    g <- ggplot(threshold_data, aes(x=.threshold, y=.estimate)) +
        geom_line() +
        geom_point(data=opt_threshold, color="red", size=2) +
        labs(title=model_name, x="Threshold", y="F-Measure") +
        coord_cartesian(ylim=c(.1, 0.9))
    return(list(
        graph=g,
        threshold=opt_threshold %>%
            pull(.threshold)
    ))
}
g1 <- threshold_scan(logreg_model, asteroid_holdout, "Logistic regression")
g2 <- threshold_scan(lda_model, asteroid_holdout, "LDA")
g3 <- threshold_scan(qda_model, asteroid_holdout, "QDA")

logreg_threshold <- g1$threshold
lda_threshold <- g2$threshold
qda_threshold <- g3$threshold

g1$graph + g2$graph + g3$graph

```

In this case it makes sense to use the F-Measure to tune the model because of the imbalance of the data set. The vast majority of the asteroids in the data set are not hazardous, as a result the model could do fairly well at classification if it simply said that *any* asteroid is not hazardous, and would be very accurate. F-Measure however considers both precision and recall. That is it looks at how many of the predicted positives are positive, and how many of the positives are correctly identified.

### 2.7 Model Performance

If we were to now use the ideal f-measure and apply it to our models, the metrics should change.

```{r}
predict_at_threshold <- function(model, data, threshold) {
    return(
        model %>%
            augment(data) %>%
            mutate(.pred_class = make_two_class_pred(.pred_FALSE,
                    c("FALSE", "TRUE"), threshold=threshold)
            )
    )
}

log_p <- predict_at_threshold(logreg_model,asteroid_holdout,threshold = logreg_threshold)
lda_p <- predict_at_threshold(lda_model,asteroid_holdout,threshold = lda_threshold)
qda_p <- predict_at_threshold(qda_model,asteroid_holdout,threshold = qda_threshold)
```

```{r}
my_metrics <- metric_set(accuracy,sensitivity,specificity,f_meas)
l_metrics <- my_metrics(log_p,truth = Hazardous,estimate=.pred_class)
ld_metrics <- my_metrics(lda_p,truth = Hazardous,estimate=.pred_class) 
q_metrics <- my_metrics(qda_p,truth = Hazardous,estimate=.pred_class)

all_new_metrics <- bind_rows(
  l_metrics %>% mutate(Model = "Log"),
  ld_metrics %>% mutate(Model = "LDA"),
  q_metrics %>% mutate(Model = "QDA")
)

metrics_table(all_new_metrics, "New Metrics")

```

Here we can see that the models have improved. While the accuracy has gone down for the models, the f-measure has increased. As we have seen f-measure is a better measure of the "goodness" of a model, when the data is significantly skewed.

\newpage

## 3. Class Imbalance

What is a classification problem, and what is a class imbalance? In general, a classification problem is a predictive modeling problem where a model tries to predict the class label for each observation. For example, we wanted to build a model that predicted whether or not an e-mail was spam or not spam. We could make a reasonably good model if we had a sufficiently large data set and enough predictors correlated with the outcome. This is an example of a binary classification model with a balanced data set, as the outcome, whether or not an e-mail is spam, is about an even split (even if it doesn't seem that). What happens if the two outcomes are not split evenly? What happens if, somehow, a bad actor sold the e-mail address in question, and now most e-mails are spam? A model could do a fairly good job classifying an e-mail as spam if it classified every e-mail as spam. This is an example of an imbalanced classification problem.[@brownlee2019][@karabibera]

Now that we understand what an imbalanced classification problem is, what are some strategies used to handle these specific class imbalance problems? Some common strategies include collecting more data, changing the performance metric, or even re-sampling the data. Each of these tries to combat the class imbalance differently.[@brownlee2015]

Collecting More Data: Maybe the data set is not large enough. If the observations are easy to come across, sampling more might be possible to change the class imbalance. However, if it is not feasible to collect more data, a different strategy may be needed.[@brownlee2015a]

Change the performance Metric: The basis for this issue is the accuracy metric. A model might easily predict the majority class; however, it will struggle with predicting the minority class. So, if we change the metric from accuracy to a different metric, such as F-Score or Kappa, we can get a model that better handles the class imbalance. F-Score takes the precision and recall of a model into account. That is, it looks at how many of the predicted positives are positive and how many of the positives are correctly identified. Kappa normalizes the accuracy of the model by the class imbalance. Both of these metrics would do a better job of tuning an imbalanced classification model.[@brownlee2015a]

Re sampling the data: We can even add copies of the underrepresented class or remove instances of the over represented class. These changes in sampling may help create a more accurate model.[@brownlee2015a]

Some combination of these methods may be used to combat an imbalanced data set. There is no one "right" way of doing it, so the best approach is to try them all and see what works. Imbalanced data sets can be a real pain when it comes to classification problems, and an effort must be made to address them.
